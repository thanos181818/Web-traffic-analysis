# üß† Hadoop Web Analytics Dashboard

A comprehensive web analytics platform that processes Apache server logs using **Hadoop MapReduce** and visualizes insights through an interactive **Flask dashboard**.

---

## üìä Features

- 6 Different Analytics on web server logs  
- Interactive Charts and visualizations  
- Real-time Data processing with Hadoop  
- Multi-tab Dashboard for different insights  
- Responsive Design ‚Äî works on all devices  

---

## üèóÔ∏è Architecture

```
Raw Logs ‚Üí Hadoop HDFS ‚Üí MapReduce Processing ‚Üí Flask Dashboard ‚Üí Interactive Visualizations
```

---

## üìÅ Project Structure

```
hadoop-web-analytics/
‚îú‚îÄ‚îÄ mapreduce_jobs/              # Java MapReduce analysis jobs
‚îÇ   ‚îú‚îÄ‚îÄ PageAnalysis.java        # Most visited pages
‚îÇ   ‚îú‚îÄ‚îÄ TrafficAnalysis.java     # Hourly traffic patterns
‚îÇ   ‚îú‚îÄ‚îÄ UserAnalysis.java        # User distribution & referrers
‚îÇ   ‚îú‚îÄ‚îÄ PeakHoursAnalysis.java   # Busiest traffic hours
‚îÇ   ‚îú‚îÄ‚îÄ ErrorRateAnalysis.java   # HTTP error analysis
‚îÇ   ‚îî‚îÄ‚îÄ ContentPerformance.java  # Content popularity by time
‚îú‚îÄ‚îÄ visualization/               # Flask web dashboard
‚îÇ   ‚îú‚îÄ‚îÄ web_dashboard.py         # Main Flask application
‚îÇ   ‚îú‚îÄ‚îÄ config.py                # Environment configuration
‚îÇ   ‚îú‚îÄ‚îÄ templates/               # HTML templates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page_views.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ traffic_analysis.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_analysis.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ peak_hours.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ error_analysis.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_performance.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ charts.html          # Interactive charts
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îú‚îÄ‚îÄ style.css            # Styling
‚îÇ       ‚îî‚îÄ‚îÄ chart.js             # Chart configurations
‚îú‚îÄ‚îÄ scripts/                     # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_all_jobs.ps1         # Run all MapReduce jobs
‚îÇ   ‚îî‚îÄ‚îÄ setup_hdfs.sh            # HDFS setup
‚îú‚îÄ‚îÄ data/                        # Sample data
‚îÇ   ‚îî‚îÄ‚îÄ log_generator.py         # Apache log generator
‚îî‚îÄ‚îÄ results/                     # Analysis outputs (generated)
```

---

## üöÄ Quick Start

### Prerequisites

- Docker Desktop  
- Git  
- Python 3.8+  

---

### 1. Start Hadoop Cluster

```bash
docker-compose up -d
sleep 30
docker ps
```

---

### 2. Initial Setup & Compilation

```bash
docker exec namenode hdfs dfs -rm -r /user/root/output/* 2>/dev/null || true
rm -rf results/
docker exec namenode rm -rf /mapreduce_jobs/classes /mapreduce_jobs/*.jar

docker exec namenode hdfs dfs -mkdir -p /user/root/output
docker exec namenode mkdir -p /mapreduce_jobs/classes
mkdir -p results
```

---

### 3. Copy and Compile Java Files

```bash
docker cp mapreduce_jobs/PageAnalysis.java namenode:/mapreduce_jobs/
docker cp mapreduce_jobs/TrafficAnalysis.java namenode:/mapreduce_jobs/
docker cp mapreduce_jobs/UserAnalysis.java namenode:/mapreduce_jobs/
docker cp mapreduce_jobs/PeakHoursAnalysis.java namenode:/mapreduce_jobs/
docker cp mapreduce_jobs/ErrorRateAnalysis.java namenode:/mapreduce_jobs/
docker cp mapreduce_jobs/ContentPerformanceAnalysis.java namenode:/mapreduce_jobs/

docker exec namenode javac -cp $(docker exec namenode hadoop classpath) -d /mapreduce_jobs/classes /mapreduce_jobs/*.java

docker exec namenode jar cf /mapreduce_jobs/pageanalysis.jar -C /mapreduce_jobs/classes PageAnalysis*.class
docker exec namenode jar cf /mapreduce_jobs/trafficanalysis.jar -C /mapreduce_jobs/classes TrafficAnalysis*.class
docker exec namenode jar cf /mapreduce_jobs/useranalysis.jar -C /mapreduce_jobs/classes UserAnalysis*.class
docker exec namenode jar cf /mapreduce_jobs/peakhours.jar -C /mapreduce_jobs/classes PeakHoursAnalysis*.class
docker exec namenode jar cf /mapreduce_jobs/errorrates.jar -C /mapreduce_jobs/classes ErrorRateAnalysis*.class
docker exec namenode jar cf /mapreduce_jobs/contentperformance.jar -C /mapreduce_jobs/classes ContentPerformanceAnalysis*.class
```

---

### 4. Run All MapReduce Analyses

```bash
docker exec namenode hadoop jar /mapreduce_jobs/pageanalysis.jar PageAnalysis /user/root/input /user/root/output/page_analysis
docker exec namenode hadoop jar /mapreduce_jobs/trafficanalysis.jar TrafficAnalysis /user/root/input /user/root/output/traffic_analysis
docker exec namenode hadoop jar /mapreduce_jobs/useranalysis.jar UserAnalysis /user/root/input /user/root/output/user_analysis
docker exec namenode hadoop jar /mapreduce_jobs/peakhours.jar PeakHoursAnalysis /user/root/input /user/root/output/peak_hours
docker exec namenode hadoop jar /mapreduce_jobs/errorrates.jar ErrorRateAnalysis /user/root/input /user/root/output/error_rates
docker exec namenode hadoop jar /mapreduce_jobs/contentperformance.jar ContentPerformanceAnalysis /user/root/input /user/root/output/content_performance
```

---

### 5. Verify Results

```bash
docker exec namenode hdfs dfs -ls /user/root/output/
docker exec namenode hdfs dfs -cat /user/root/output/page_analysis/part-r-00000 | head -5
docker exec namenode hdfs dfs -cat /user/root/output/traffic_analysis/part-r-00000 | head -5
docker exec namenode hdfs dfs -cat /user/root/output/user_analysis/part-r-00000 | head -5
```

---

### 6. Copy Results Locally (Optional)

```bash
docker exec namenode hdfs dfs -get /user/root/output/* /tmp/
docker cp namenode:/tmp/ ./results/
```

---

### 7. Start Web Dashboard

```bash
cd visualization
pip install flask
python web_dashboard.py
```

---

### 8. Access the Dashboard

Visit:
```
http://localhost:5000
```

---

## üìà Available Analyses

- **Page Views** ‚Äì Most visited pages and content popularity  
- **Traffic Analysis** ‚Äì Hourly patterns and status codes  
- **User Analysis** ‚Äì Visitor distribution and referral sources  
- **Peak Hours** ‚Äì Busiest traffic periods and patterns  
- **Error Rates** ‚Äì HTTP errors and problematic pages  
- **Content Performance** ‚Äì Page popularity across time slots  
- **Interactive Charts** ‚Äì Visual representations of all data  

---

## üõ†Ô∏è Development

### Environment Configuration

The application automatically detects the environment:

- **Development:** Uses local files, runs on `localhost:5000`  
- **Production:** Uses HDFS directly, runs on `0.0.0.0:5000`  

Set environment variable:
```bash
export FLASK_ENV=production  # or development
```

---

### Regenerating Sample Data

```bash
cd data
python log_generator.py
```

---

### Restarting Services

```bash
docker-compose down
docker-compose up -d
cd visualization
python web_dashboard.py
```

---

## üîß Troubleshooting

### Common Issues

#### Docker containers not starting
```bash
docker-compose down
docker system prune -a
docker-compose up -d
```

#### HDFS connection issues
```bash
docker exec namenode hdfs dfs -ls /
```

#### Web dashboard not loading data
Visit:  
[http://localhost:5000/status](http://localhost:5000/status)

#### Port 5000 already in use
```bash
app.run(debug=True, host='localhost', port=8000)
```

---

## üìä Hadoop Components

- **namenode:** HDFS master server  
- **datanode:** Data storage nodes  
- **resourcemanager:** YARN resource management  
- **nodemanager:** Node task management  
- **historyserver:** Job history tracking  

---

## üåê Web URLs

| Service | URL |
|----------|-----|
| **Dashboard** | [http://localhost:5000](http://localhost:5000) |
| **HDFS UI** | [http://localhost:9870](http://localhost:9870) |
| **YARN UI** | [http://localhost:8088](http://localhost:8088) |
| **Job History** | [http://localhost:8188](http://localhost:8188) |
| **System Status** | [http://localhost:5000/status](http://localhost:5000/status) |

---

